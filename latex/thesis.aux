\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{stat}
\citation{Lisanti2009}
\citation{Candes2007}
\citation{stat}
\citation{stat}
\citation{Candes2006}
\citation{Mamoshina2016}
\citation{Fakoor2013}
\citation{Zeng2015}
\citation{Cox1999}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Number of magnetic resonance imaging (MRI) scan examinations in Germany from 2008 to 2015 \cite  {stat}\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mri_stat}{{1.1}{4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}MRI}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Basics}{5}}
\newlabel{eq:larmor}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Pulse sequence}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Spatial encoding}{6}}
\citation{Encoding2019}
\citation{Encoding2019}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Gradients for slice selection, frequency encoding and phase encoding are utilized to locate signals in 3D space. \cite  {Encoding2019}\relax }}{7}}
\newlabel{fig:mri-encoding}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Slice selection}{7}}
\citation{Cutcliffe1988}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces MR pulse timing diagram with simplified rectangular gradient representation. The phase of the proton in the gradient is permanently shifted $(\Phi )$ after the gradient is applied, whereas the reference proton's phase does not shift.\relax }}{8}}
\newlabel{fig:phase_encoding}{{2.2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Frequency encoding}{8}}
\citation{Felmlee1989}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Phase encoding}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}k-space}{9}}
\citation{Lustig2007}
\citation{Lustig2007}
\citation{Lustig2007}
\newlabel{eq:ifft}{{2.11}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Compressed Sensing}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Transform Sparsity}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Incoherence of Undersampling Artifacts}{10}}
\citation{Lustig2007}
\citation{Natarajan1995}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison between $\ell _0$, $\ell _1$ and $\ell _2$ norm for a minimization problem in the form of $\qopname  \relax m{min}\left \delimiter 69645069 \boldsymbol  {w} \right \delimiter 86422285 $ such that $\boldsymbol  {Aw}=\boldsymbol  {y}$. Blue points indicate constant values of $\left \delimiter 69645069 \boldsymbol  {w} \right \delimiter 86422285 $ for the corresponding norm. Red points are solutions of $\boldsymbol  {Aw}=\boldsymbol  {y}$. Note that the red line $\boldsymbol  {y}$ intersects the $\ell _2$ circle at a non-sparse point, while $\ell _0$ and $\ell _1$ both enforce $w_2$ to be $0$.\relax }}{11}}
\newlabel{fig:sparsity_l1}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Nonlinear Reconstruction}{11}}
\newlabel{eq:cs}{{2.14}{11}}
\citation{Krahmer2017}
\citation{Krahmer2017}
\citation{Krahmer2017}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The original Fabio image (left) and the absolute values after application of a discrete gradient operator(right) \cite  {Krahmer2017}.\relax }}{12}}
\newlabel{fig:fabio}{{2.4}{12}}
\newlabel{eq:cs_tv}{{2.16}{12}}
\citation{Werner2016}
\citation{Werner2016}
\citation{Hansen1990}
\citation{Werner2016}
\citation{Karlik2011}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Multilayer perceptron network \cite  {Werner2016}.\relax }}{13}}
\newlabel{fig:nn}{{2.5}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Basics}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Feedforward propagation}{13}}
\newlabel{eq:1}{{2.18}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Activation functions}{13}}
\citation{Hornik1989}
\citation{Maas2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Three different activation functions\relax }}{14}}
\newlabel{fig:activation}{{2.6}{14}}
\newlabel{eq:2}{{2.19}{14}}
\newlabel{eq:3}{{2.20}{14}}
\citation{Werner2016}
\newlabel{eq:4}{{2.21}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Training}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Objective}{15}}
\newlabel{cost1}{{2.22}{15}}
\newlabel{cost2}{{2.23}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Steepest descent}{15}}
\newlabel{eq:taylor}{{2.24}{15}}
\citation{Hinton}
\citation{Basu2018}
\citation{ChandraMukkamala2017}
\citation{Hinton}
\citation{Hinton}
\citation{a}
\citation{Hinton}
\newlabel{eq:cost_func}{{2.25}{16}}
\newlabel{eq:steepest}{{2.26}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline RMSProp}{16}}
\citation{Sutskever2013}
\citation{Hinton}
\citation{Srivastava2014}
\citation{Srivastava2014}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Momentum Method}{17}}
\newlabel{eq:classical_momentum}{{2.29}{17}}
\newlabel{eq:mom}{{2.30}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Dropout}{17}}
\citation{Srivastava2014}
\citation{b}
\citation{Srivastava2014}
\citation{Dreyfus2005}
\newlabel{fig:drop1}{{2.7a}{18}}
\newlabel{sub@fig:drop1}{{a}{18}}
\newlabel{fig:drop2}{{2.7b}{18}}
\newlabel{sub@fig:drop2}{{b}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \textbf  {Left:} Neural network with one hidden layer \textbf  {Right:} Example of a thinned neural network produced by applying dropout with $p_{keep}=0.5$ to the network on the left. Crossed out units and their connections are dropped.\relax }}{18}}
\newlabel{fig:dropout}{{2.7}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Early Stopping}{18}}
\citation{Bengio2015}
\citation{Zhu2018}
\citation{Zhu2018}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2.1}{\ignorespaces Pseudocode for early stopping\relax }}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}AUTOMAP}{19}}
\newlabel{automap}{{2.3.3}{19}}
\citation{Zhu2018}
\citation{Zhu2018}
\citation{Zhu2018}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Graphical representation of AUTOMAP neural network layers\relax }}{20}}
\newlabel{fig:automap}{{2.8}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Network architecture}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Training details}{20}}
\citation{Wang2004}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Evaluation}{21}}
\newlabel{eq:ssim_0}{{2.33}{21}}
\citation{Wang2004}
\citation{ab}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Example images with equal MSE and different SSIM. \textbf  {Left:} Original image \textbf  {Middle:} Image with noise \textbf  {Right:} Image with added constant, which is chosen to match MSE of the image with noise. In this case the SSIM represents the perceived quality much better than the MSE.\relax }}{22}}
\newlabel{fig:ssim}{{2.9}{22}}
\newlabel{eq:ssim}{{2.37}{22}}
\citation{Zhu2018}
\citation{Deng2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Experimental Setup}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Overview of performed experiments.\relax }}{23}}
\newlabel{tab:Overview of performed experiments.}{{3.1}{23}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Data preparation}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Signal processing chain from raw image data to final image reconstruction. The dashed line describes data flow that only appears during neural network training.\relax }}{24}}
\newlabel{fig:whole_setup}{{3.1}{24}}
\citation{Zhang2016}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Input data preprocessing}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Log-transformation}{25}}
\newlabel{Log-transformation}{{3.1.1}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf  {Top row:} Scaling factors $s_{lin}$ and $s_{log}$ in two different number ranges. \textbf  {Bottom row:} Functions $f_{lin}$ and $f_{log}$ in two different number ranges. The bigger the absolute value of x, the smaller the scaling factor.\relax }}{26}}
\newlabel{fig:log_2}{{3.2}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Normalization technique}{27}}
\newlabel{eq:min_max_normalization}{{3.3}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Normalization basis}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Phase preservation}{27}}
\newlabel{Phase preservation}{{3.1.4}{27}}
\citation{Zhu2018}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Undersampling masks}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}AUTOMAP hyperparameter tuning}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Sampling masks where black pixels include and white pixels exclude k-space data samples.\relax }}{29}}
\newlabel{fig:pattern}{{3.3}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Performance comparison}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Input data preprocessing}{31}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces AUTOMAP MSEs with differently normalized data-sets. Default hyperparameter with learning rate of $2\cdot 10^{-5}$, multiplicative noise on k-space data input, no dropout and no momentum are utilized. K-space input data is fully sampled.\relax }}{31}}
\newlabel{tab:result_1}{{4.1}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Undersampling masks}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces AUTOMAP MSEs with different undersampling masks. Default hyperparameter with learning rate of $2\cdot 10^{-5}$, multiplicative noise on k-space data input, no dropout and no momentum are utilized. The amount of k-space undersampling is determined by the reduction factor.\relax }}{32}}
\newlabel{tab:table1}{{4.2}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}AUTOMAP hyperparameter tuning}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Top 10 AUTOMAP MSEs with different hyperparameters.\relax }}{33}}
\newlabel{tab:table_tuning}{{4.3}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Performance comparison}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces AUTOMAP MSEs with different hyper parameters. These violin plots show minimum, median and maximum by horizontal line marks and the whole sample distribution by the shape for each hyper parameter (shape width correlates with number of samples for the corresponding MSE).\relax }}{34}}
\newlabel{fig:automap_tuning}{{4.1}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Reconstruction metric results for CS and AUTOMAP\relax }}{34}}
\newlabel{tab:final_results}{{4.4}{34}}
\newlabel{tab:comparison_1}{{\caption@xref {tab:comparison_1}{ on input line 1066}}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Reconstructions of the brain image and four images from the test-set. The absolute errors range from 0 (black) to 0.25 (white) and the local SSIMs range from 1 (white) to 0.75 (black) to improve visualization.\relax }}{35}}
\newlabel{fig:comparison_1}{{4.2}{35}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Performance}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison of required time to reconstruct images between CPU and GPU.\relax }}{38}}
\newlabel{fig:cpu_gpu}{{5.1}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces MSE comparison between our and Nature paper reconstruction results.\relax }}{38}}
\newlabel{fig:nature_comparison}{{5.2}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Prediction errors during training for training- validation- and test-set for the final neural network.\relax }}{39}}
\newlabel{fig:pred_errors}{{5.3}{39}}
\citation{Zhu2018}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Memory demand}{40}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Number of weights and biases for AUTOMAP\relax }}{41}}
\newlabel{tab:weights_biases}{{5.1}{41}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{43}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{stat}
\citation{Encoding2019}
\citation{Krahmer2017}
\citation{Werner2016}
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{Mamoshina2016}{1}
\bibcite{Fakoor2013}{2}
\bibcite{Zeng2015}{3}
\bibcite{Cox1999}{4}
\bibcite{stat}{5}
\bibcite{Lisanti2009}{6}
\bibcite{Candes2007}{7}
\bibcite{Candes2006}{8}
\bibcite{Encoding2019}{9}
\bibcite{Cutcliffe1988}{10}
\bibcite{Felmlee1989}{11}
\bibcite{Lustig2007}{12}
\bibcite{Natarajan1995}{13}
\bibcite{Krahmer2017}{14}
\bibcite{Werner2016}{15}
\bibcite{Hansen1990}{16}
\bibcite{Karlik2011}{17}
\bibcite{Hornik1989}{18}
\bibcite{Maas2013}{19}
\bibcite{Hinton}{20}
\bibcite{Basu2018}{21}
\bibcite{ChandraMukkamala2017}{22}
\bibcite{a}{23}
\bibcite{Sutskever2013}{24}
\bibcite{Srivastava2014}{25}
\bibcite{b}{26}
\bibcite{Dreyfus2005}{27}
\bibcite{Bengio2015}{28}
\bibcite{Zhu2018}{29}
\bibcite{Wang2004}{30}
\bibcite{ab}{31}
\bibcite{Deng2009}{32}
\bibcite{Zhang2016}{33}
\@writefile{toc}{\contentsline {chapter}{Appendices}{53}}
\@writefile{toc}{\setcounter {tocdepth}{0}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Reconstruction examples}{53}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Reconstructions of five images from the test-set. The absolute errors range from 0 (black) to 0.25 (white) and the local SSIMs range from 1 (white) to 0.75 (black) to improve visualization.\relax }}{54}}
\newlabel{fig:comparison_2}{{A.1}{54}}
\global\@altsecnumformattrue
